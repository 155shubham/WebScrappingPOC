from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup

import time
import json


def scrape_certificates_data():
    """Scrape certificates data from the ISCC website."""

    # Add the headless options so that it will run in background when Azure function will execute this script
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    # chrome_options.add_argument("--no-sandbox")
    # chrome_options.add_argument("--disable-dev-shm-usage")
    driver = webdriver.Chrome(options=chrome_options)

    # Configurations --- this has to be brought from key vault
    website = {
        "ISCC": "https://www.iscc-system.org/certification/certificate-database/all-certificates/"}

    # Navigate to the website.
    driver.get(website["ISCC"])

    time.sleep(10)

    # Create a list to store the certificates data.
    certificates_data = []
    page_no = 2
    dynamic_page_no = 2
    while True:
        soup = BeautifulSoup(driver.page_source, "html.parser")
        table = soup.find("table", {"id": "table_1"})

        # print(table)

        if table is not None:
            for row in table.find_all("tr"):
                cols = row.find_all("td")
                certificates_data.append([col.text for col in cols])
        else:
            print(
                f"Certificates Table not found on this website {website['ISCC']}")
            break

        nextpage_no_xpath = f"//*[@id='table_1_paginate']/span/a[{dynamic_page_no}]"
        nextpage_no_element = driver.find_element(
            by=By.XPATH, value=nextpage_no_xpath)

        if nextpage_no_element is not None:
            nextpage_no_element.click()
            time.sleep(1)
        else:
            print(f"Page No {page_no} in table table_1 not present")
            break

        if page_no >= 5:
            dynamic_page_no = 4
        else:
            dynamic_page_no += 1

        page_no += 1

        if page_no == 60:
            break

    # Close the browser.
    driver.close()

    # Save the certificates data to a JSON file.
    json_data = json.dumps(certificates_data, indent=4)

    with open("iscc_certificates.json", "w") as f:
        f.write(json_data)


if __name__ == "__main__":
    scrape_certificates_data()
