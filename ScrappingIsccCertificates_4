import selenium
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import Select

import time
from bs4 import BeautifulSoup

import json


def scrape_certificates_data():
    """Scrape certificates data from the ISCC website."""

    chrome_options = Options()
    chrome_options.add_argument("--headless")
    # chrome_options.add_argument("--no-sandbox")
    # chrome_options.add_argument("--disable-dev-shm-usage")
    driver = webdriver.Chrome(options=chrome_options)

    # Navigate to the website.
    driver.get(
        "https://www.iscc-system.org/certification/certificate-database/all-certificates/")

    time.sleep(10)

    # dropdown = Select(driver.find_element(
    #     By.XPATH("//*[@id='table_1_length']/label/div/button")))
    # dropdown.select_by_value(100)

    # nextpage_no_xpath = "//a[@class='paginate_button '][@data-dt-idx='4']"
    nextpage_no_xpath = "//*[@id='table_1_paginate']/span/a[4]"

    nextpage_no_element = driver.find_element(
        by=By.XPATH, value=nextpage_no_xpath)

    if nextpage_no_element:
        nextpage_no_element.click()
    else:
        print("Next Page not found")

    time.sleep(10)

    soup = BeautifulSoup(driver.page_source, "html.parser")
    table = soup.find("table", {"id": "table_1"})

    print(table)

    # Create a list to store the certificates data.
    certificates_data = []
    if table is not None:
        for row in table.find_all("tr"):
            cols = row.find_all("td")
            certificates_data.append([col.text for col in cols])
    else:
        print("Table not found on this page")

    # # Loop through all the pages.
    # while True:
    #     # Find the tables on the page.
    #     table = driver.find_elements(by=By.ID, value="table_1")

    #     soup = BeautifulSoup(table)

    #     # Scrape the data from the tables.
    #     if table is not None:
    #         certificate_data = {}
    #         for row in table.find_all("tr"):
    #             cols = row.find_all("td")
    #             for col in cols:
    #                 certificate_data[col.get("data-label")] = col.text

    #         certificates_data.append(certificate_data)

    #         # Find the button to go to the next page.
    #         next_page_button = driver.find_element(by=By.XPATH,
    #                                                value="//a[@class='paginate_button '][@data-dt-idx='4']")

    #         # Click the button to go to the next page.
    #         if next_page_button:
    #             next_page_button.click()
    #         else:
    #             break
    #     else:
    #         print("Table not found")

    # Close the browser.
    driver.close()

    # Save the certificates data to a JSON file.
    json_data = json.dumps(certificates_data, indent=4)

    with open("iscc_certificates.json", "w") as f:
        f.write(json_data)

    # # Save the certificates data to a JSON file.
    # with open("certificates_data.json", "w") as f:
    #     json.dump(certificates_data, f)


if __name__ == "__main__":
    scrape_certificates_data()
